"""
llm.py

üìå Fun√ß√£o deste arquivo:
Conectar com o LLM (ex.: OpenAI GPT) para tomar decis√£o sobre um ProcessoMinimo.

‚û° Entrada: ProcessoMinimo (j√° normalizado pelo preprocessador).
‚û° Sa√≠da: RespostaDecisao (decis√£o final + justificativa + cita√ß√µes).

-------------------------------------------------------------------------------
üìå Como funciona:
1. Monta o prompt com:
   - Texto da pol√≠tica (POL-1 a POL-8).
   - Estrutura m√≠nima do processo (JSON).
2. Envia para o modelo da OpenAI (com formato for√ßado em JSON).
3. Recebe e valida a resposta convertendo em RespostaDecisao.
"""

import json
import os
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()

from .modelos import ProcessoMinimo, RespostaDecisao
from .politica import POLITICA_TEXTO

# Modelo padr√£o (pode ser trocado no .env)
MODEL = os.getenv("OPENAI_MODEL")

# Mensagem de sistema para instruir o LLM
SYSTEM_MSG = """
# Sua caracter√≠stica profissional

- Voc√™ √© um verificador jur√≠dico altamente criterioso e profissional.
- Sua fun√ß√£o √© aplicar exclusivamente as regras fornecidas da pol√≠tica,sem interpreta√ß√µes 
al√©m do que est√° explicitamente descrito.
"""

def montar_prompt(proc: ProcessoMinimo) -> str:
    """
    Cria o prompt com:
    # - Regras de decis√£o
    # - Texto da pol√≠tica (IDs + descri√ß√µes)
    # - Estrutura m√≠nima do processo em JSON
    """


    return f"""

# Instru√ß√µes

- Aplique exclusivamente as regras definidas na pol√≠tica.
- Respeite a seguinte ordem de prioridade:

  1. **Regras de rejei√ß√£o absoluta** (POL-3, POL-4, POL-5, POL-6).  
     - Se alguma delas for satisfeita, o resultado deve ser **"rejected"**, mesmo que tamb√©m faltem documentos.  
     - Nesses casos, n√£o use POL-8 na cita√ß√£o.
     - "resultado": "rejected",

  2. **Regras de aprova√ß√£o m√≠nima** (POL-1 e POL-2).  
     - Considere como documentos essenciais: Transito Julgado e Cumprimento Definitivo Iniciado. (POL-1) e valor da condena√ß√£o (POL-2). 
     - Se um deles faltar, o processo deve ser **"incomplete"** com as cita√ß√µes correspondentes acrescida do (POL-8).

  3. **Regras de documento essencial** (POL-8).  
     - S√≥ aplique o POL-8 quando faltar documento essencial (Transito Julgado e Cumprimento Definitivo Iniciado. (POL-1) e valor da condena√ß√£o (POL-2).).  
     - "resultado": "incomplete",
     
  4. **Regras de honor√°rios obrigat√≥rios** (POL-7).  
     - Use POL-7 quando existir informa√ß√µes de honor√°rios  

# Resposta
 
- A resposta deve ser em **JSON v√°lido**, contendo:
  - resultado: apenas um dos valores em ingl√™s: "approved", "rejected" ou "incomplete"
  - justificativa: explica√ß√£o clara e objetiva do motivo da decis√£o
  - citacoes: lista com todos os IDs aplicados (ex.: ["POL-1", "POL-2"])

# Pol√≠tica
(use sempre os IDs exatamente como definidos)

{POLITICA_TEXTO}

# Decis√£o
Analise o processo abaixo e devolva SOMENTE o JSON solicitado. N√£o inclua nenhum texto fora do JSON.

PROCESSO_MINIMO:
{proc.model_dump_json(indent=2, exclude_none=True)}
"""

def decidir_com_llm(proc: ProcessoMinimo) -> RespostaDecisao:
    """
    Envia o ProcessoMinimo para o LLM e retorna uma RespostaDecisao validada.
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    prompt = montar_prompt(proc)

    resp = client.chat.completions.create(
        model=MODEL,
        response_format={"type": "json_object"},  # for√ßa sa√≠da em JSON
        messages=[
            {"role": "system", "content": SYSTEM_MSG},
            {"role": "user", "content": prompt},
        ],
        temperature=0
    )

    conteudo = resp.choices[0].message.content

    try:
        dados = json.loads(conteudo)
    except json.JSONDecodeError:
        raise ValueError(f"Resposta inv√°lida do LLM: {conteudo}")

    return RespostaDecisao.model_validate(dados)

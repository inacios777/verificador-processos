"""

üìå Fun√ß√£o deste arquivo:

Integrar o sistema ao LLM da OpenAI, enviando os dados j√° normalizados (ProcessoMinimo)
junto com as regras da pol√≠tica e recebendo a decis√£o estruturada em formato JSON.

------------------------------------------------------------------------------------------------------------------------
Principais responsabilidades:

Configura√ß√£o do modelo:
‚û° O modelo a ser usado √© definido via vari√°vel de ambiente (OPENAI_MODEL).
‚û° A chave da API (OPENAI_API_KEY) tamb√©m √© carregada do .env via dotenv.

Mensagem de sistema (SYSTEM_MSG):
‚û° Define o comportamento esperado do LLM: um verificador jur√≠dico rigoroso, que aplica apenas as regras
fornecidas sem interpreta√ß√µes adicionais.

Montagem do prompt (montar_prompt):
‚û° Estrutura o contexto enviado ao LLM contendo:
Instru√ß√µes claras de prioridade das regras (rejei√ß√£o absoluta ‚Üí aprova√ß√£o m√≠nima ‚Üí completude ‚Üí honor√°rios).
Texto integral da pol√≠tica (POLITICA_TEXTO).
Estrutura m√≠nima do processo em JSON (ProcessoMinimo).
‚û° Refor√ßa que a sa√≠da deve ser exclusivamente JSON v√°lido com os campos resultado, justificativa e citacoes.

Decis√£o com LLM (decidir_com_llm):
‚û° Constr√≥i o cliente da OpenAI, envia o prompt e recebe a resposta.
‚û° Garante sa√≠da em JSON usando response_format={"type": "json_object"}.
‚û° Valida a resposta contra o modelo RespostaDecisao para assegurar conformidade.
‚û° Caso a sa√≠da n√£o seja JSON v√°lido, levanta exce√ß√£o (ValueError).

"""

import json
import os
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()

from .modelos import ProcessoMinimo, RespostaDecisao
from .politica import POLITICA_TEXTO

# Modelo padr√£o (pode ser trocado no .env)
MODEL = os.getenv("OPENAI_MODEL")

# Mensagem de sistema para instruir o LLM
SYSTEM_MSG = """
# Sua caracter√≠stica profissional

- Voc√™ √© um verificador jur√≠dico altamente criterioso e profissional.
- Sua fun√ß√£o √© aplicar exclusivamente as regras fornecidas da pol√≠tica,sem interpreta√ß√µes 
al√©m do que est√° explicitamente descrito.
"""

def montar_prompt(proc: ProcessoMinimo) -> str:
    """
    Cria o prompt com:
    # - Regras de decis√£o
    # - Texto da pol√≠tica (IDs + descri√ß√µes)
    # - Estrutura m√≠nima do processo em JSON
    """


    return f"""

# Instru√ß√µes

- Aplique exclusivamente as regras definidas na pol√≠tica.
- Respeite a seguinte ordem de prioridade:

  1. **Regras de rejei√ß√£o absoluta** (POL-3, POL-4, POL-5, POL-6).  
     - Se alguma delas for satisfeita, o resultado deve ser **"rejected"**, mesmo que tamb√©m faltem documentos.  
     - Nesses casos, n√£o use POL-8 na cita√ß√£o.
     - "resultado": "rejected",

  2. **Regras de aprova√ß√£o m√≠nima** (POL-1 e POL-2).  
     - Considere como documentos essenciais: Transito Julgado e Cumprimento Definitivo Iniciado. (POL-1) e valor da condena√ß√£o (POL-2). 
     - Se um deles faltar, o processo deve ser **"incomplete"** com as cita√ß√µes correspondentes acrescida do (POL-8).

  3. **Regras de documento essencial** (POL-8).  
     - S√≥ aplique o POL-8 quando faltar documento essencial (Transito Julgado e Cumprimento Definitivo Iniciado. (POL-1) e valor da condena√ß√£o (POL-2).).  
     - "resultado": "incomplete",
     
  4. **Regras de honor√°rios obrigat√≥rios** (POL-7).  
     - Use POL-7 quando existir informa√ß√µes de honor√°rios  

# Resposta
 
- A resposta deve ser em **JSON v√°lido**, contendo:
  - resultado: apenas um dos valores em ingl√™s: "approved", "rejected" ou "incomplete"
  - justificativa: explica√ß√£o clara e objetiva do motivo da decis√£o
  - citacoes: lista com todos os IDs aplicados (ex.: ["POL-1", "POL-2"])

# Pol√≠tica
(use sempre os IDs exatamente como definidos)

{POLITICA_TEXTO}

# Decis√£o
Analise o processo abaixo e devolva SOMENTE o JSON solicitado. N√£o inclua nenhum texto fora do JSON.

PROCESSO_MINIMO:
{proc.model_dump_json(indent=2, exclude_none=True)}
"""

def decidir_com_llm(proc: ProcessoMinimo) -> RespostaDecisao:
    """
    Envia o ProcessoMinimo para o LLM e retorna uma RespostaDecisao validada.
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    prompt = montar_prompt(proc)

    resp = client.chat.completions.create(
        model=MODEL,
        response_format={"type": "json_object"},  # for√ßa sa√≠da em JSON
        messages=[
            {"role": "system", "content": SYSTEM_MSG},
            {"role": "user", "content": prompt},
        ],
        temperature=0
    )

    conteudo = resp.choices[0].message.content

    try:
        dados = json.loads(conteudo)
    except json.JSONDecodeError:
        raise ValueError(f"Resposta inv√°lida do LLM: {conteudo}")

    return RespostaDecisao.model_validate(dados)
